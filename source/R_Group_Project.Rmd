---
title: "Project 1"
author: "Name:  \n Partner: "
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage{threeparttablex}
  - \usepackage[normalem]{ulem}
  - \usepackage{makecell}
  - \usepackage{xcolor}
---
```{r setup, include = FALSE}
#### Load necessary packages ####
# * These packages are not necessary to complete the assignment and or only used 
#   to provide an example. 
packages <- c("knitr", "kableExtra", "magrittr", "readr", "geosphere", "dplyr")
install_me <- packages[!(packages %in% installed.packages()[, "Package"])]
if (length(install_me)) install.packages(install_me)
library(knitr)
#library(kableExtra)
library(magrittr)
library(readr)
library(geosphere)
library(dplyr)
```

## Background
The World Health Organization has recently employed a new data science initiative, *CSIT-165*, that uses data science to characterize pandemic diseases. 
*CSIT-165* disseminates data driven analyses to global decision makers.

*CSIT-165* is a conglomerate comprised of two fabricated entities: *Global Health Union (GHU)* and *Private Diagnostic Laboratories (PDL)*. 
Your and your partner's role is to play a data scientist from one of these two entities.

## Data
> [2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository by John Hopkins CSSE](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series)
Data for 2019 Novel Coronavirus is operated by the John Hopkins University Center for Systems Science and Engineering (JHU CSSE).
Data includes daily time series CSV summary tables, including confirmations, recoveries, and deaths. 
Country/region are countries/regions hat conform to World Health Organization (WHO).
Lat and Long refer to coordinates references for the user. 
Date fields are stored in MM/DD/YYYY format.

## Project Objectives
######Make function definition file#############
######Make data grab happen in one place#######
### Objective 1
```{r ob1, echo=T, include=TRUE}
# Algorithm 
# Step1. Read in csv files.
# Step2. find which column is the first day.
# Step3. Make a list of the first days places as keys and with a count as the value
# Step4. go through the list and look for the max count, the corresponding location is the origin.
# Step5. Print the location.
add_first_day_cases <- function(lst, data_frame){
  for (i in 1:nrow(data_frame))
  {
    #key will be the name of the place without spaces added.
    key <- paste0(data_frame[ i, 1], " ", data_frame[ i, 2])
    
    num_of_cases <- data_frame[ i, 5]
    
    if (num_of_cases > 0 && is.null(lst[[key]]))
    {
      lst[[key]] <- num_of_cases
    }
    else if (num_of_cases > 0)
    {
      lst[[key]] <- lst[[key]] + num_of_cases
    }
    else
    {
      #do nothing
    }
  }
  lst # return lst
}

# read in data
confirmed <- read.csv(file="../data/time_series_covid19_confirmed_global.csv", stringsAsFactors = FALSE)

recovered <- read.csv(file="../data/time_series_covid19_recovered_global.csv", stringsAsFactors = FALSE)

deaths <- read.csv(file="../data/time_series_covid19_deaths_global.csv", stringsAsFactors = FALSE)

first_day_cases <- list()

# call function for each data set.
first_day_cases <- add_first_day_cases(first_day_cases, confirmed)

first_day_cases <- add_first_day_cases(first_day_cases, recovered)

first_day_cases <- add_first_day_cases(first_day_cases, deaths)

#Output answer
places <- names(first_day_cases)

ground_zero <- places[which.max(first_day_cases)]

print(paste0("Ground zero for the virus seems to be ", ground_zero, "."))


```
Objective 1, Answer: We believe this to be the origin because it has the maximum number of cases on day 0. The dataset begins with the recorded cases starting with date 1/22/20, which is after the first case globally was found. The day that the first case for any area was recorded is not included within the dataset. To find the most likely area to have the global first case it is believed that the area would have the most cases by 1/22/20. See code comments for how the number of cases was computed. 

### Objective 2
```{r ob2, echo=T, include=TRUE}
confirmed <- read.csv(file="../data/time_series_covid19_confirmed_global.csv", stringsAsFactors = FALSE)

#Dataframe to contain Providence and Country for the places with the most recent first cases
recent_places <- data.frame(Providence = nrow(confirmed), Country = ncol(confirmed)) 

y <- 1

#iterates through rows
for(x in 1:nrow(confirmed)){ 
  
  #Start at the last column
  i <- ncol(confirmed) 
  
  #Values for the number of cases
  new_cases <- confirmed[x,i] 
  
  #Values for the number of cases the day before
  cases_before <- confirmed[x,(i-1)]
  
  #The first date of instance
  if (new_cases > 0 && cases_before == 0){ 
    
    recent_places[y,1:2] <- confirmed[x,1:2]
    
    y <- y + 1
    
  } else {
    
  #Move back one column to find previous day if no matches found
  i <- i - 1 
  
  }
}

#Change dataframe to correct size
recent_places <- recent_places[1:(y-1),] 

print(recent_places)
```

### Objective 3
```{r ob3, echo=T, include=TRUE}

confirmed <- read.csv(file="../data/time_series_covid19_confirmed_global.csv", stringsAsFactors = FALSE)

#Dataframe to contain Providence, Country, Lat and Long for the places with the most recent first cases
recent_places <- data.frame(Providence = nrow(confirmed), Country = ncol(confirmed), Lat = nrow(confirmed), Long = ncol(confirmed)) 

y <- 1

#iterates through rows
for(x in 1:nrow(confirmed)){ 
  
  #Start at the last column
  i <- ncol(confirmed) 
  
  #Values for the number of cases
  new_cases <- confirmed[x,i] 
  
  #Values for the number of cases the day before
  cases_before <- confirmed[x,(i-1)]
  
  #The first date of instance
  if (new_cases > 0 && cases_before == 0){ 
    
    recent_places[y,1:4] <- confirmed[x,1:4]
    
    y <- y + 1
    
  } else {
    
  #Move back one column to find previous day if no matches found
  i <- i - 1 
  
  }
}

#Change dataframe to correct size
recent_places <- recent_places[1:(y-1),] 

y <- 1

first_place <- data.frame(Providence = 1, Country = 1, Lat = 1, Long = 1)

cases_number <- 0

for (x in 1:nrow(confirmed)) {
 
  #Start at first column of case number data 
  i <- 5 
  
  #Values for the number of cases
  cases <- confirmed[x,i] 
  
  #Largest number of cases on earliest day reported
  if (cases > cases_number) 
    
    {
    first_place[1,1:4] <- confirmed[x,1:4] 
    
    cases_number <- confirmed[x,i]
    }
  else 
    
  {
    #Move to next day if no cases found
    i <- i + 1 
    
  }
}
#Distance in meters from origin
dist_meter <- geosphere::distm(recent_places[,4:3], first_place[1,4:3], fun = distGeo) 

#Distance in miles from origin
dist_mile <- dist_meter/1609.344 

#Dataframe with print information
full_info <- data.frame(recent_places[,2], dist_mile[], first_place[,1:2]) 

#Order by distance from least to greatest
ordered_info <- full_info[order(dist_mile),] 

cat(paste0(ordered_info[,1], " is ", ordered_info[,2], " miles away from ", ordered_info[,3], ", ", ordered_info[,4], sep="\n"))

```

### Objective 4

#### Objective 4.1
<<<<<<< HEAD
```{r ob4.1, echo=T, include=TRUE}
library("dplyr")
=======
```{r ob4.1}
>>>>>>> 287e54f727ccc46296145770e6e8c442e4e0b8ba
#Algorithm
#Step1. Find the risk for each area
#Step2. Find the burden for each area
#Step3. Find global risk and confirmed

# read in data
confirmed <- read.csv(file="../data/time_series_covid19_confirmed_global.csv", stringsAsFactors = FALSE)

recovered <- read.csv(file="../data/time_series_covid19_recovered_global.csv", stringsAsFactors = FALSE)

deaths <- read.csv(file="../data/time_series_covid19_deaths_global.csv", stringsAsFactors = FALSE)

#Find global data
global_confirmations <- sum(confirmed[, length(confirmed)])

global_recoveries <- sum(recovered[, length(recovered)])

global_deaths <- sum(deaths[, length(deaths)])

global_risk <- global_deaths / global_recoveries

global_burden <- global_confirmations * global_risk

modified_confirmed <- left_join(recovered, confirmed, by = c("Province.State", "Country.Region"))
modified_deaths <- left_join(recovered, deaths, by = c("Province.State", "Country.Region"))

risk <- modified_deaths[, ncol(modified_deaths)] / recovered[ , ncol(recovered)]

burden <- modified_confirmed[, ncol(modified_confirmed)] * risk

recovered_with_scores <- mutate(
  recovered,
  risk_score = risk,
  burden_score = burden
)

#Pick only the 4 neccassary columns. 
mins_risk <- select(
  filter(recovered_with_scores, risk_score == min(recovered_with_scores$risk_score, na.rm = TRUE)), 
  Province.State, Country.Region, risk_score, burden_score
)

maxs_risk <- select(
  filter(recovered_with_scores, risk_score == max(recovered_with_scores$risk_score, na.rm = TRUE, inf.rm = TRUE)), 
  Province.State, Country.Region, risk_score, burden_score
)

#Output this is the data to read
print(paste0("The global risk is ", global_risk))

print(paste0("The global burden is ", global_burden))

mins_risk

maxs_risk
```
Objective 4.1 Questions
Question 1. China has the lowest risk rate. We believe this could be because they are ground zero, thus allowing more people to recover and thus driving down the risk rate. Furthermore, China has enacted many containment policies. The data may also not be accurate due to reasons unknown to us statisticians in training. 

Question 2. So the places with the highest risk scores are the Pacific Islands and some African nations and islands. It seems places highest are the places with the least exposure. 

Question 3. The low risk scores are pretty small and close to the global risk score. However, the risk scores of the maxs is not close to the global risk score. They could be outliers. 

Question 4. The least risks and most risks places of the world have the same burden scores as their risk scores. This occured because burden is the sum of confirmed times the risk score, thus if you have zero risk you will have zero burden, and if you have infinite risk you will have infinite burden. This also makes sense analytically since burden corresponds to risk.

Question 5. Some of these values seem reasonable for example not have any risk because nobody has died yet. But, having infinite risk because we performed a divide by zero because no one has recovered yet. 

Question 6. As already stated this discrepancy occurs when a zero appears in the denonimator.Or when a data point is missing. Or when we have a NaN in the data set.

Question 7. It could be helpful to tell the the ratio of deaths to recoveries. However, it does not account for other factors such as people who have it and to not report or people who do not know that they have it and recover. The final thing is that the report relies on honesty and willing to share data, and all players sadly are not.

#### Objective 4.2
<<<<<<< HEAD
```{r ob4.2, echo=T, include=TRUE}
install.packages("dplyr")
library("dplyr")
=======
```{r ob4.2}
>>>>>>> 287e54f727ccc46296145770e6e8c442e4e0b8ba

#Generates total data frames
generate_total_df <- function(places, sums, column_name)
{
  countries <- vector()
  
  total_cases <- vector()
  
  for (i in 1:length(places)){
    
    if (is.element(places[i], countries)){
      
      new <- match(places[i], countries)
      
      total_cases[new] <- total_cases[new] + sums[i] 
      
    }
    else{
      
      countries <- c(countries, places[i])
      
      total_cases <- c(total_cases, sums[i])
      
    }
  }
  
  total_cases <- data.frame(countries, total_cases)
  
  colnames(total_cases) <- c("countries", column_name)
  
  total_cases
  
}

#Read input from csv files

confirmed <- read.csv(file="../data/time_series_covid19_confirmed_global.csv", stringsAsFactors = FALSE)

recovered <- read.csv(file="../data/time_series_covid19_recovered_global.csv", stringsAsFactors = FALSE)

deaths <- read.csv(file="../data/time_series_covid19_deaths_global.csv", stringsAsFactors = FALSE)

#calculate the sums of each

sums_confirmed <- confirmed[, ncol(confirmed)]

sums_recoveries <- recovered[, ncol(recovered)]

sums_deaths <- deaths[, ncol(deaths)]

#Generate the totals

total_confirmations <- generate_total_df(confirmed$Country.Region, sums_confirmed, "total_confirmations")

total_recoveries <- generate_total_df(recovered$Country.Region,  sums_recoveries, "total_recoveries")

total_deaths <- generate_total_df(deaths$Country.Region, sums_deaths, "total_deaths")

#Sort the data_frames
total_confirmations_sorted <- arrange(total_confirmations, -total_confirmations)

total_recoveries_sorted <-arrange(total_recoveries, -total_recoveries)

total_deaths_sorted <- arrange(total_deaths, -total_deaths)

#Output the data_frames as a table

kable(total_confirmations_sorted[1:5,])

kable(total_recoveries_sorted[1:5,])

kable(total_deaths_sorted[1:5,])

```

### GitHub Log
```{bash gitlog} 
git log --pretty=format:"%nSubject: %s%nAuthor: %aN%nDate: %aD%nBody: %b"
```





© 2020 GitHub, Inc.
Terms
Privacy
Security
Status
Help
Contact GitHub
Pricing
API
Training
Blog
About
